{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import GPy\n",
    "\n",
    "l_test_set_path = \"./StarLightCurves/StarLightCurves_TEST\"\n",
    "m_test_set_path = \"./MALLAT/MALLAT_TEST\"\n",
    "\n",
    "# two trained HGP models\n",
    "l_model_class_names = [1,3]\n",
    "m_model_class_names = [3,6]\n",
    "\n",
    "# perc of Testing data used for HGP sample size\n",
    "l_sample_size_perc = 0.5\n",
    "m_sample_size_perc = 0.5\n",
    "\n",
    "# outlier class names\n",
    "l_outlier_class_names = [2]\n",
    "m_outlier_class_names = [7]\n",
    "\n",
    "# perc of normal and abnormal items\n",
    "# normal_prec + outlier_prec = 1\n",
    "# normal_perc = 0.95\n",
    "# total_test_prec = 0.9\n",
    "\n",
    "\n",
    "outlier_prec = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Rebbapragada, 2008) mentioned that the shapes of CEPH and RRL are very similar, so they referred to CEPH and RRL as normal classes, and EB was regarded as the anomaly for the known anomalies test. In MALLAT dataset, class 3,6 are normal classes, and class 7,2 are abnormal ones. In this way, they compared the performance of PCAD (their method) with other methods (K-means etc.).\n",
    "\n",
    "- S1: train two HGP models (model 1 uses CEPH and RRL as training dataset; model 2 uses MALLAT class 3,6 as training dataset)\n",
    "- S2: build two testing datasets including 90% normal objectives and 10% outliers for OGLE and MALLAT respectively.\n",
    "- S2: apply model 1 and model 2 to the testing datasets, and measure precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datapre: light_curve->l; MALLAT->m\n",
    "def importModel(datapre, model_class_names):\n",
    "    if datapre == 'l':\n",
    "        X_file_path = \"./X_files/X_\" + str(model_class_names)  + \".npy\"\n",
    "        Y_file_path = \"./Y_files/Y_\" + str(model_class_names)  + \".npy\"\n",
    "        model_path = \"./model_save_files/model_save\" + str(model_class_names) + \".npy\"\n",
    "    else:\n",
    "        X_file_path = \"./MALLAT_files/X_files/X_\" + str(model_class_names)  + \".npy\"\n",
    "        Y_file_path = \"./MALLAT_files/Y_files/Y_\" + str(model_class_names)  + \".npy\"\n",
    "        model_path = \"./MALLAT_files/model_save_files/model_save\" + str(model_class_names) + \".npy\"\n",
    "\n",
    "    X_load = np.load(X_file_path)\n",
    "    Y_load = np.load(Y_file_path)\n",
    "\n",
    "\n",
    "    kern_class = GPy.kern.Matern32(input_dim=1, variance=1.5, lengthscale=2.5, active_dims=[0], name='class')\n",
    "    kern_replicate = GPy.kern.Matern32(input_dim=1, variance=.1, lengthscale=2.5, active_dims=[0], name='replicate')\n",
    "    k_hierarchy = GPy.kern.Hierarchical(kernels=[kern_class, kern_replicate])\n",
    "    \n",
    "    m_load = GPy.models.GPRegression(X_load, Y_load, initialize=False, kernel=k_hierarchy)\n",
    "    m_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "    m_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "    m_load[:] = np.load(model_path) # Load the parameters\n",
    "    m_load.update_model(True) # Call the algebra only once\n",
    "    print(m_load)\n",
    "    return m_load, kern_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\capec\\Anaconda3\\lib\\site-packages\\paramz\\parameterized.py:57: RuntimeWarning:Don't forget to initialize by self.initialize_parameter()!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : GP regression\n",
      "Objective : -46428.07763277821\n",
      "Number of Parameters : 5\n",
      "Number of Optimization Parameters : 5\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.                 \u001b[0;0m  |              value  |  constraints  |  priors\n",
      "  \u001b[1mhierarchy.class.variance       \u001b[0;0m  |      2.71174780166  |      +ve      |        \n",
      "  \u001b[1mhierarchy.class.lengthscale    \u001b[0;0m  |      3.82722758939  |      +ve      |        \n",
      "  \u001b[1mhierarchy.replicate.variance   \u001b[0;0m  |    0.0230718947076  |      +ve      |        \n",
      "  \u001b[1mhierarchy.replicate.lengthscale\u001b[0;0m  |      62.6156237403  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance        \u001b[0;0m  |  5.09238969154e-41  |      +ve      |        \n"
     ]
    }
   ],
   "source": [
    "# S1: we've already trained those models and saved in files\n",
    "# import hgp models\n",
    "l_m, l_kern_class = importModel('l', l_model_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\capec\\Anaconda3\\lib\\site-packages\\paramz\\parameterized.py:57: RuntimeWarning:Don't forget to initialize by self.initialize_parameter()!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : GP regression\n",
      "Objective : -3294.161055100078\n",
      "Number of Parameters : 5\n",
      "Number of Optimization Parameters : 5\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.                 \u001b[0;0m  |              value  |  constraints  |  priors\n",
      "  \u001b[1mhierarchy.class.variance       \u001b[0;0m  |     0.268871188474  |      +ve      |        \n",
      "  \u001b[1mhierarchy.class.lengthscale    \u001b[0;0m  |      15.0932921491  |      +ve      |        \n",
      "  \u001b[1mhierarchy.replicate.variance   \u001b[0;0m  |    0.0799247892631  |      +ve      |        \n",
      "  \u001b[1mhierarchy.replicate.lengthscale\u001b[0;0m  |      36.3341124539  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance        \u001b[0;0m  |  4.41307397404e-16  |      +ve      |        \n"
     ]
    }
   ],
   "source": [
    "m_m, m_kern_class = importModel('m', m_model_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectTopNormalIndex(model_class_names, top_rate):\n",
    "    top_index = []\n",
    "    for m_c in model_class_names:\n",
    "        m_c_index_load = np.loadtxt(\"sorted_result_model-\"+str(m_c)+\"class-\"+str(m_c)+\".csv\", delimiter=',', usecols=[0])\n",
    "        m_c_top_rows_num = int(len(m_c_index_load) * top_rate)\n",
    "        m_c_top_index_list = m_c_index_load[:m_c_top_rows_num:].tolist()\n",
    "        top_index.extend(m_c_top_index_list)\n",
    "    \n",
    "#     print(\"top_index length=\",len(top_index))\n",
    "    return top_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTestData(test_set_path):\n",
    "    class_names_test = np.loadtxt(test_set_path, delimiter=',', usecols=[0])\n",
    "    test_data = np.loadtxt(test_set_path, delimiter=',', usecols=range(1, 1025))\n",
    "    test_data -= test_data.mean(1)[:,np.newaxis]\n",
    "    test_data /= test_data.std(1)[:,np.newaxis]\n",
    "    \n",
    "    return class_names_test,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample_prec doesn't work now\n",
    "def generateTestData(class_test_names, sample_prec, model_class_names, outlier_class_names, normal_index_range=[],total_test_num=500):\n",
    "#     print(outlier_class_names)\n",
    "    if(normal_index_range):\n",
    "        normal_indices = normal_index_range\n",
    "    else:\n",
    "        normal_indices = [i for i,cn in enumerate(class_test_names) if cn in model_class_names]\n",
    "    abnormal_indices = [i for i,cn in enumerate(class_test_names) if cn in outlier_class_names]\n",
    "    \n",
    "#     total_test_indices_num = int((len(normal_indices) + len(abnormal_indices)) * total_test_prec)\n",
    "# #     print(abnormal_indices)\n",
    "\n",
    "#     normal_num = int(normal_perc * total_test_indices_num)\n",
    "#     abnormal_num = int(outlier_prec * total_test_indices_num)\n",
    "#     print(normal_num,abnormal_num)\n",
    "#     print(len(normal_indices))\n",
    "\n",
    "    normal_num = int((1-outlier_prec) * total_test_num)\n",
    "    abnormal_num = int(outlier_prec * total_test_num)\n",
    "    \n",
    "    sample_normal_indices = sample(normal_indices, normal_num)\n",
    "    sample_normal_indices = np.asarray(sample_normal_indices)\n",
    "    sample_normal_indices = sample_normal_indices.reshape(-1,1)\n",
    "    \n",
    "    \n",
    "    sample_abnormal_indices = sample(abnormal_indices, abnormal_num)\n",
    "    sample_abnormal_indices = np.asarray(sample_abnormal_indices)\n",
    "    sample_abnormal_indices = sample_abnormal_indices.reshape(-1,1)\n",
    "    \n",
    "#     print(sample_normal_indices, sample_abnormal_indices)\n",
    "    \n",
    "    # normal->0; abnormal->1\n",
    "    normal_indicator = np.zeros(len(sample_normal_indices))\n",
    "    normal_indicator = normal_indicator.reshape(-1,1)\n",
    "    \n",
    "    abnormal_indicator = np.ones(len(sample_abnormal_indices))\n",
    "    abnormal_indicator = abnormal_indicator.reshape(-1,1)\n",
    "    \n",
    "#     print(sample_normal_indices.shape, normal_indicator.shape)\n",
    "#     print(sample_abnormal_indices.shape, abnormal_indicator.shape)\n",
    "    normal_array = np.concatenate((sample_normal_indices, normal_indicator), axis=1)\n",
    "#     print(normal_array)\n",
    "    abnormal_array = np.concatenate((sample_abnormal_indices, abnormal_indicator), axis=1)\n",
    "#     print(abnormal_array)\n",
    "    conbine_array = np.concatenate((normal_array, abnormal_array), axis=0)\n",
    "#     print(conbine_array)\n",
    "    \n",
    "    return conbine_array, normal_num, abnormal_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calLikelihood(test_data, indices, m_load, kern_class):\n",
    "    indices = indices.tolist()\n",
    "    log_pre_density_result = np.ones(len(indices)) * 9999\n",
    "    log_pre_density_result = log_pre_density_result.reshape(-1,1)\n",
    "    x_test = np.arange(1,1025)[:,None]\n",
    "    mu_star, var_star = m_load.predict_noiseless(x_test, kern=kern_class)\n",
    "    \n",
    "    for index in range(len(indices)):\n",
    "        y_test = test_data[int(indices[index]),:].reshape(-1,1)\n",
    "        log_pre_density_result[index] = np.average(m_load.likelihood.log_predictive_density(y_test, mu_star, var_star))\n",
    "        \n",
    "    return log_pre_density_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_result_array format\n",
    "\n",
    "| index                               | indicator               | log_predictive_density |\n",
    "|-------------------------------------|-------------------------|:----------------------:|\n",
    "| sampled indexes of  testing dataset | normal(0) / abnormal(1) |                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sortLikelihood(test_array, likelihood):\n",
    "    likelihood = np.asarray(likelihood)\n",
    "    likelihood = likelihood.reshape(-1,1)\n",
    "\n",
    "    combine_result = np.concatenate((test_array, likelihood), axis=1)\n",
    "    sorted_result = np.sort(combine_result.view('f8,f8,f8'), order=['f2'], axis=0).view(np.float)\n",
    "    \n",
    "#     print(sorted_result)\n",
    "    \n",
    "#     result_file_name = \"sorted_result_model.csv\";\n",
    "#     np.savetxt(result_file_name, sorted_result, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calPrecision(sorted_result_array, abnormal_num):\n",
    "    detect_abnormal_num_array = sorted_result_array[0:abnormal_num,:]\n",
    "#     print(detect_abnormal_num_array.shape)\n",
    "    detect_abnormal_num = np.sum(detect_abnormal_num_array[:,1])\n",
    "    return detect_abnormal_num / abnormal_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_array format\n",
    "\n",
    "| index                               | indicator               |\n",
    "|-------------------------------------|-------------------------|\n",
    "| sampled indexes of  testing dataset | normal(0) / abnormal(1) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# S2: build testing datasets\n",
    "l_class_names_test, l_test_data = processTestData(l_test_set_path)\n",
    "m_class_names_test, m_test_data = processTestData(m_test_set_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_num= 5000  normal_num= 4500  abnormal_num= 500  precision= 0.988\n"
     ]
    }
   ],
   "source": [
    "top_likelihood_rows_rate = 0.8\n",
    "total_test_num = 5000\n",
    "    # S3: applying HGP models to each testing dataset\n",
    "l_normal_index_range = selectTopNormalIndex(l_model_class_names, top_likelihood_rows_rate)\n",
    "l_normal_index_range = [int(i) for i in l_normal_index_range]\n",
    "\n",
    "l_test_array, l_normal_num, l_abnormal_num = generateTestData(l_class_names_test, l_sample_size_perc, l_model_class_names, l_outlier_class_names, l_normal_index_range,total_test_num)\n",
    "l_likelihood = calLikelihood(l_test_data, l_test_array[:,0], l_m, l_kern_class)\n",
    "l_sorted_result_array = sortLikelihood(l_test_array, l_likelihood)\n",
    "np.savetxt(\"l_sorted_result_array.csv\", l_sorted_result_array, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "l_precision = calPrecision(l_sorted_result_array, l_abnormal_num)\n",
    "print('total_test_num=',total_test_num,' normal_num=',l_normal_num,' abnormal_num=',l_abnormal_num,' precision=',l_precision)\n",
    "    \n",
    "    \n",
    "#     m_test_array, m_normal_num, m_abnormal_num = generateTestData(m_class_names_test, m_sample_size_perc, m_model_class_names, m_outlier_class_names,[],total_test_num)\n",
    "#     m_likelihood = calLikelihood(m_test_data, m_test_array[:,0], m_m, m_kern_class)\n",
    "#     m_sorted_result_array = sortLikelihood(m_test_array, m_likelihood)\n",
    "#     np.savetxt(\"m_sorted_result_array.csv\", m_sorted_result_array, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "#     m_precision = calPrecision(m_sorted_result_array, m_abnormal_num)\n",
    "#     print('total_test_num=',total_test_num,' normal_num=',m_normal_num,' abnormal_num=',m_abnormal_num,' precision=',m_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_num= 10  normal_num= 9  abnormal_num= 1  precision= 1.0\n",
      "total_test_num= 20  normal_num= 18  abnormal_num= 2  precision= 1.0\n",
      "total_test_num= 30  normal_num= 27  abnormal_num= 3  precision= 1.0\n",
      "total_test_num= 40  normal_num= 36  abnormal_num= 4  precision= 1.0\n",
      "total_test_num= 50  normal_num= 45  abnormal_num= 5  precision= 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d3984b2eaf08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0ml_test_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_normal_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_abnormal_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateTestData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_class_names_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_sample_size_perc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_model_class_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_outlier_class_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_normal_index_range\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal_test_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0ml_likelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalLikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_test_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_test_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_kern_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0ml_sorted_result_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msortLikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_test_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_likelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"l_sorted_result_array.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_sorted_result_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%d,%d,%1.9f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-73ab90fe54e5>\u001b[0m in \u001b[0;36mcalLikelihood\u001b[1;34m(test_data, indices, m_load, kern_class)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlog_pre_density_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_pre_density_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmu_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_noiseless\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkern_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\capec\\Anaconda3\\lib\\site-packages\\GPy\\core\\gp.py\u001b[0m in \u001b[0;36mpredict_noiseless\u001b[1;34m(self, Xnew, full_cov, Y_metadata, kern)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mNote\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictive\u001b[0m \u001b[0mquantiles\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;36m95\u001b[0m\u001b[1;33m%\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0muse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"~GPy.core.gp.GP.predict_quantiles\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_cov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_metadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_quantiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m97.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\capec\\Anaconda3\\lib\\site-packages\\GPy\\core\\gp.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, Xnew, full_cov, Y_metadata, kern, likelihood, include_likelihood)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m#predict the latent function values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_cov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfull_cov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minclude_likelihood\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\capec\\Anaconda3\\lib\\site-packages\\GPy\\core\\gp.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[1;34m(self, Xnew, full_cov, kern)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0;31m\\\u001b[0m\u001b[0mSigma\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mtexttt\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mLikelihood\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariance\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mApproximate\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[0mcovariance\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposterior\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkern\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkern\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXnew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_var\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictive_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_cov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfull_cov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mmu\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\capec\\Anaconda3\\lib\\site-packages\\GPy\\inference\\latent_function_inference\\posterior.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[1;34m(self, kern, Xnew, pred_var, full_cov)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0mKxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_woodbury_chol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m                 \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtrtrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_woodbury_chol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m                 \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKxx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_woodbury_chol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Missing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "top_likelihood_rows_rate = 0.8\n",
    "\n",
    "for total_test_num in range(10,660,10):   \n",
    "    # S3: applying HGP models to each testing dataset\n",
    "    l_normal_index_range = selectTopNormalIndex(l_model_class_names, top_likelihood_rows_rate)\n",
    "    l_normal_index_range = [int(i) for i in l_normal_index_range]\n",
    "    \n",
    "    l_test_array, l_normal_num, l_abnormal_num = generateTestData(l_class_names_test, l_sample_size_perc, l_model_class_names, l_outlier_class_names, l_normal_index_range,total_test_num)\n",
    "    l_likelihood = calLikelihood(l_test_data, l_test_array[:,0], l_m, l_kern_class)\n",
    "    l_sorted_result_array = sortLikelihood(l_test_array, l_likelihood)\n",
    "    np.savetxt(\"l_sorted_result_array.csv\", l_sorted_result_array, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "    l_precision = calPrecision(l_sorted_result_array, l_abnormal_num)\n",
    "    print('total_test_num=',total_test_num,' normal_num=',l_normal_num,' abnormal_num=',l_abnormal_num,' precision=',l_precision)\n",
    "    \n",
    "    \n",
    "#     m_test_array, m_normal_num, m_abnormal_num = generateTestData(m_class_names_test, m_sample_size_perc, m_model_class_names, m_outlier_class_names,[],total_test_num)\n",
    "#     m_likelihood = calLikelihood(m_test_data, m_test_array[:,0], m_m, m_kern_class)\n",
    "#     m_sorted_result_array = sortLikelihood(m_test_array, m_likelihood)\n",
    "#     np.savetxt(\"m_sorted_result_array.csv\", m_sorted_result_array, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "#     m_precision = calPrecision(m_sorted_result_array, m_abnormal_num)\n",
    "#     print('total_test_num=',total_test_num,' normal_num=',m_normal_num,' abnormal_num=',m_abnormal_num,' precision=',m_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
