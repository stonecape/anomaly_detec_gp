{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import GPy\n",
    "\n",
    "l_test_set_path = \"../StarLightCurves/StarLightCurves_TEST\"\n",
    "m_test_set_path = \"../MALLAT/MALLAT_TEST\"\n",
    "\n",
    "# two trained HGP models\n",
    "l_model_class_names = [1,3]\n",
    "m_model_class_names = [3,6]\n",
    "\n",
    "# perc of Testing data used for HGP sample size\n",
    "l_sample_size_perc = 0.5\n",
    "m_sample_size_perc = 0.5\n",
    "\n",
    "# outlier class names\n",
    "l_outlier_class_names = [2]\n",
    "m_outlier_class_names = [7]\n",
    "\n",
    "# perc of normal and abnormal items\n",
    "# normal_prec + outlier_prec = 1\n",
    "# normal_perc = 0.95\n",
    "# total_test_prec = 0.9\n",
    "\n",
    "\n",
    "outlier_prec = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Rebbapragada, 2008) mentioned that the shapes of CEPH and RRL are very similar, so they referred to CEPH and RRL as normal classes, and EB was regarded as the anomaly for the known anomalies test. In MALLAT dataset, class 3,6 are normal classes, and class 7,2 are abnormal ones. In this way, they compared the performance of PCAD (their method) with other methods (K-means etc.).\n",
    "\n",
    "- S1: train two HGP models (model 1 uses CEPH and RRL as training dataset; model 2 uses MALLAT class 3,6 as training dataset)\n",
    "- S2: build two testing datasets including 90% normal objectives and 10% outliers for OGLE and MALLAT respectively.\n",
    "- S2: apply model 1 and model 2 to the testing datasets, and measure precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datapre: light_curve->l; MALLAT->m\n",
    "def importModel(datapre, model_class_names, sample_size):\n",
    "    if datapre == 'l':\n",
    "        X_file_path = \"./light_curve_model_files/X_\" + str(model_class_names) + str(sample_size) + \".npy\"\n",
    "        Y_file_path = \"./light_curve_model_files/Y_\" + str(model_class_names)  + str(sample_size) + \".npy\"\n",
    "        model_path = \"./light_curve_model_files/model_save\" + str(model_class_names) + str(sample_size) + \".npy\"\n",
    "    else:\n",
    "        X_file_path = \"./mallat_model_files/X_\" + str(model_class_names) + str(sample_size) + \".npy\"\n",
    "        Y_file_path = \"./mallat_model_files/Y_\" + str(model_class_names) + str(sample_size) + \".npy\"\n",
    "        model_path = \"./mallat_model_files/model_save\" + str(model_class_names) + str(sample_size) + \".npy\"\n",
    "\n",
    "    X_load = np.load(X_file_path)\n",
    "    Y_load = np.load(Y_file_path)\n",
    "\n",
    "\n",
    "    kern_class = GPy.kern.Matern32(input_dim=1, variance=1.5, lengthscale=2.5, active_dims=[0], name='class')\n",
    "    kern_replicate = GPy.kern.Matern32(input_dim=1, variance=.1, lengthscale=2.5, active_dims=[0], name='replicate')\n",
    "    k_hierarchy = GPy.kern.Hierarchical(kernels=[kern_class, kern_replicate])\n",
    "    \n",
    "    m_load = GPy.models.GPRegression(X_load, Y_load, initialize=False, kernel=k_hierarchy)\n",
    "    m_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "    m_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "    m_load[:] = np.load(model_path) # Load the parameters\n",
    "    m_load.update_model(True) # Call the algebra only once\n",
    "#     print(m_load)\n",
    "    return m_load, kern_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectTopNormalIndex(model_class_names, top_rate):\n",
    "    top_index = []\n",
    "    for m_c in model_class_names:\n",
    "        m_c_index_load = np.loadtxt(\"sorted_result_model-\"+str(m_c)+\"class-\"+str(m_c)+\".csv\", delimiter=',', usecols=[0])\n",
    "        m_c_top_rows_num = int(len(m_c_index_load) * top_rate)\n",
    "        m_c_top_index_list = m_c_index_load[:m_c_top_rows_num:].tolist()\n",
    "        top_index.extend(m_c_top_index_list)\n",
    "    \n",
    "#     print(\"top_index length=\",len(top_index))\n",
    "    return top_index\n",
    "\n",
    "\n",
    "def processTestData(test_set_path):\n",
    "    class_names_test = np.loadtxt(test_set_path, delimiter=',', usecols=[0])\n",
    "    test_data = np.loadtxt(test_set_path, delimiter=',', usecols=range(1, 1025))\n",
    "    test_data -= test_data.mean(1)[:,np.newaxis]\n",
    "    test_data /= test_data.std(1)[:,np.newaxis]\n",
    "    \n",
    "    return class_names_test,test_data\n",
    "\n",
    "\n",
    "# sample_prec doesn't work now\n",
    "def generateTestData(class_test_names, sample_prec, model_class_names, outlier_class_names, normal_index_range=[],total_test_num=500):\n",
    "#     print(outlier_class_names)\n",
    "    if(normal_index_range):\n",
    "        normal_indices = normal_index_range\n",
    "    else:\n",
    "        normal_indices = [i for i,cn in enumerate(class_test_names) if cn in model_class_names]\n",
    "    abnormal_indices = [i for i,cn in enumerate(class_test_names) if cn in outlier_class_names]\n",
    "    \n",
    "#     total_test_indices_num = int((len(normal_indices) + len(abnormal_indices)) * total_test_prec)\n",
    "# #     print(abnormal_indices)\n",
    "\n",
    "#     normal_num = int(normal_perc * total_test_indices_num)\n",
    "#     abnormal_num = int(outlier_prec * total_test_indices_num)\n",
    "#     print(normal_num,abnormal_num)\n",
    "#     print(len(normal_indices))\n",
    "\n",
    "    normal_num = int((1-outlier_prec) * total_test_num)\n",
    "    abnormal_num = int(outlier_prec * total_test_num)\n",
    "    \n",
    "    sample_normal_indices = sample(normal_indices, normal_num)\n",
    "    sample_normal_indices = np.asarray(sample_normal_indices)\n",
    "    sample_normal_indices = sample_normal_indices.reshape(-1,1)\n",
    "    \n",
    "    \n",
    "    sample_abnormal_indices = sample(abnormal_indices, abnormal_num)\n",
    "    sample_abnormal_indices = np.asarray(sample_abnormal_indices)\n",
    "    sample_abnormal_indices = sample_abnormal_indices.reshape(-1,1)\n",
    "    \n",
    "#     print(sample_normal_indices, sample_abnormal_indices)\n",
    "    \n",
    "    # normal->0; abnormal->1\n",
    "    normal_indicator = np.zeros(len(sample_normal_indices))\n",
    "    normal_indicator = normal_indicator.reshape(-1,1)\n",
    "    \n",
    "    abnormal_indicator = np.ones(len(sample_abnormal_indices))\n",
    "    abnormal_indicator = abnormal_indicator.reshape(-1,1)\n",
    "    \n",
    "#     print(sample_normal_indices.shape, normal_indicator.shape)\n",
    "#     print(sample_abnormal_indices.shape, abnormal_indicator.shape)\n",
    "    normal_array = np.concatenate((sample_normal_indices, normal_indicator), axis=1)\n",
    "#     print(normal_array)\n",
    "    abnormal_array = np.concatenate((sample_abnormal_indices, abnormal_indicator), axis=1)\n",
    "#     print(abnormal_array)\n",
    "    conbine_array = np.concatenate((normal_array, abnormal_array), axis=0)\n",
    "#     print(conbine_array)\n",
    "    \n",
    "    return conbine_array, normal_num, abnormal_num\n",
    "\n",
    "def sortLikelihood(test_array, likelihood):\n",
    "    likelihood = np.asarray(likelihood)\n",
    "    likelihood = likelihood.reshape(-1,1)\n",
    "\n",
    "    combine_result = np.concatenate((test_array, likelihood), axis=1)\n",
    "    sorted_result = np.sort(combine_result.view('f8,f8,f8'), order=['f2'], axis=0).view(np.float)\n",
    "    \n",
    "#     print(sorted_result)\n",
    "    \n",
    "#     result_file_name = \"sorted_result_model.csv\";\n",
    "#     np.savetxt(result_file_name, sorted_result, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "    return sorted_result\n",
    "\n",
    "def calLikelihood(test_data, indices, m_load, kern_class):\n",
    "    indices = indices.tolist()\n",
    "    log_pre_density_result = np.ones(len(indices)) * 9999\n",
    "    log_pre_density_result = log_pre_density_result.reshape(-1,1)\n",
    "    x_test = np.arange(1,1025)[:,None]\n",
    "    mu_star, var_star = m_load.predict_noiseless(x_test, kern=kern_class)\n",
    "    \n",
    "    for index in range(len(indices)):\n",
    "        y_test = test_data[int(indices[index]),:].reshape(-1,1)\n",
    "        log_pre_density_result[index] = np.average(m_load.likelihood.log_predictive_density(y_test, mu_star, var_star))\n",
    "        \n",
    "    return log_pre_density_result\n",
    "\n",
    "\n",
    "def calPrecision(sorted_result_array, abnormal_num):\n",
    "    detect_abnormal_num_array = sorted_result_array[0:abnormal_num,:]\n",
    "#     print(detect_abnormal_num_array.shape)\n",
    "    detect_abnormal_num = np.sum(detect_abnormal_num_array[:,1])\n",
    "    return detect_abnormal_num / abnormal_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\capec\\Anaconda3\\lib\\site-packages\\paramz\\parameterized.py:57: RuntimeWarning:Don't forget to initialize by self.initialize_parameter()!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : GP regression\n",
      "Objective : -13152.052365915548\n",
      "Number of Parameters : 5\n",
      "Number of Optimization Parameters : 5\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.                 \u001b[0;0m  |              value  |  constraints  |  priors\n",
      "  \u001b[1mhierarchy.class.variance       \u001b[0;0m  |  8.19095859429e-09  |      +ve      |        \n",
      "  \u001b[1mhierarchy.class.lengthscale    \u001b[0;0m  |      21.9860116073  |      +ve      |        \n",
      "  \u001b[1mhierarchy.replicate.variance   \u001b[0;0m  |      2.61135544218  |      +ve      |        \n",
      "  \u001b[1mhierarchy.replicate.lengthscale\u001b[0;0m  |      295.626067312  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance        \u001b[0;0m  |  1.8018422551e-120  |      +ve      |        \n",
      "sample_size= 0.08  total_test_num= 5000  normal_num= 4500  abnormal_num= 500  precision= 0.662\n"
     ]
    }
   ],
   "source": [
    "# S1: we've already trained those models and saved in files\n",
    "# import hgp models\n",
    "total_test_num = 5000\n",
    "top_likelihood_rows_rate = 0.8\n",
    "# for sample_size in [0.01,0.02,0.05,0.07,0.08,0.1]:\n",
    "for sample_size in [0.08]:\n",
    "    l_m, l_kern_class = importModel('l', l_model_class_names, sample_size)\n",
    "    print(l_m)\n",
    "    # S2: build testing datasets\n",
    "    l_class_names_test, l_test_data = processTestData(l_test_set_path)\n",
    "\n",
    "    # S3: applying HGP models to each testing dataset\n",
    "    l_normal_index_range = selectTopNormalIndex(l_model_class_names, top_likelihood_rows_rate)\n",
    "    l_normal_index_range = [int(i) for i in l_normal_index_range]\n",
    "\n",
    "    l_test_array, l_normal_num, l_abnormal_num = generateTestData(l_class_names_test, l_sample_size_perc, l_model_class_names, l_outlier_class_names, l_normal_index_range,total_test_num)\n",
    "    l_likelihood = calLikelihood(l_test_data, l_test_array[:,0], l_m, l_kern_class)\n",
    "    l_sorted_result_array = sortLikelihood(l_test_array, l_likelihood)\n",
    "    np.savetxt(\"l_sorted_result_array.csv\", l_sorted_result_array, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "    l_precision = calPrecision(l_sorted_result_array, l_abnormal_num)\n",
    "    print('sample_size=',sample_size,' total_test_num=',total_test_num,' normal_num=',l_normal_num,' abnormal_num=',l_abnormal_num,' precision=',l_precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\capec\\Anaconda3\\lib\\site-packages\\paramz\\parameterized.py:57: RuntimeWarning:Don't forget to initialize by self.initialize_parameter()!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size= 0.1  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 0.966666666667\n",
      "sample_size= 0.2  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 0.3  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 0.4  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 0.5  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 0.6  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 0.7  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 0.8  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 0.9  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n",
      "sample_size= 1  total_test_num= 600  normal_num= 540  abnormal_num= 60  precision= 1.0\n"
     ]
    }
   ],
   "source": [
    "total_test_num = 600\n",
    "# for sample_size in [0.01,0.02,0.05,0.07,0.08,0.1]:\n",
    "for sample_size in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    m_m, m_kern_class = importModel('m', m_model_class_names,sample_size)\n",
    "    m_class_names_test, m_test_data = processTestData(m_test_set_path)\n",
    "    \n",
    "    m_test_array, m_normal_num, m_abnormal_num = generateTestData(m_class_names_test, m_sample_size_perc, m_model_class_names, m_outlier_class_names,[],total_test_num)\n",
    "    m_likelihood = calLikelihood(m_test_data, m_test_array[:,0], m_m, m_kern_class)\n",
    "    m_sorted_result_array = sortLikelihood(m_test_array, m_likelihood)\n",
    "    np.savetxt(\"m_sorted_result_array.csv\", m_sorted_result_array, delimiter=\",\",fmt='%d,%d,%1.9f')\n",
    "    m_precision = calPrecision(m_sorted_result_array, m_abnormal_num)\n",
    "    print('sample_size=',sample_size,' total_test_num=',total_test_num,' normal_num=',m_normal_num,' abnormal_num=',m_abnormal_num,' precision=',m_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_result_array format\n",
    "\n",
    "| index                               | indicator               | log_predictive_density |\n",
    "|-------------------------------------|-------------------------|:----------------------:|\n",
    "| sampled indexes of  testing dataset | normal(0) / abnormal(1) |                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_array format\n",
    "\n",
    "| index                               | indicator               |\n",
    "|-------------------------------------|-------------------------|\n",
    "| sampled indexes of  testing dataset | normal(0) / abnormal(1) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
